# ============================================================
# Experimental: Complex-valued attention
# Usage: llm-train configs/experiment_complex_attn.yaml
# ============================================================

train_data: data/tokens/train
val_data:   data/tokens/val
tokenizer_path: gpt2

model:
  vocab_size: 50257
  d_model:    256
  num_layers: 4
  max_seq_len: 1024

  attention:
    variant:   complex    # complex-valued Hermitian attention
    num_heads: 4

  positional:
    encoding: rope        # complex RoPE phase rotation

  ffn:
    variant: swiglu

  norm:
    type: rmsnorm

seq_len:    512
batch_size: 64
gradient_accumulation_steps: 1
num_steps:  5000

optimizer:
  lr: 3.0e-4
  weight_decay: 0.1

scheduler:
  type:         cosine
  warmup_steps: 200

dtype: bfloat16

checkpoint_dir: checkpoints/complex_attn
save_every:     1000
keep_last_n:    2
log_every:      10
eval_every:     500

seed: 42
